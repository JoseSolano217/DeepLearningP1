{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feeling_on_twitter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBTrI84jPL1z"
      },
      "source": [
        "#Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CiV2tfM1yRx"
      },
      "source": [
        "##Pyprind"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbmU9VDaVwvJ",
        "outputId": "cd1d87ca-820e-445c-8b30-e1b9dbdcf3d6"
      },
      "source": [
        "!pip install scikit-learn==0.18.2\n",
        "!pip install pyprind\n",
        "!pip install scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.18.2\n",
            "  Using cached https://files.pythonhosted.org/packages/26/c2/21c612f3a1b1ba97b7b4bbd1fcdc59b475a09e25efad13fec4565ab9d563/scikit-learn-0.18.2.tar.gz\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for scikit-learn\n",
            "Failed to build scikit-learn\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.18.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.18.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 has requirement scikit-learn>=0.19.1, but you'll have scikit-learn 0.18.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "    Running setup.py install for scikit-learn ... \u001b[?25l\u001b[?25herror\n",
            "  Rolling back uninstall of scikit-learn\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/scikit_learn-0.22.2.post1.dist-info/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~cikit_learn-0.22.2.post1.dist-info\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/sklearn/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~klearn\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-b7w3fu71/scikit-learn/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-b7w3fu71/scikit-learn/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-3tqq9xgl/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.7/dist-packages (2.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOYB_l1Pzf2Q"
      },
      "source": [
        "##Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31s48wSVD60E"
      },
      "source": [
        "from google.colab import drive\n",
        "import os   \n",
        "import sys        \n",
        "import tarfile   \n",
        "import time         \n",
        "import re       \n",
        "import pyprind\n",
        "import numpy as np\n",
        "import tweepy as tw\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import nltk\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer   \n",
        "from sklearn.pipeline import Pipeline                \n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV              \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from os import scandir\n",
        "\n",
        "#from vectorizer import vect\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import xml.etree.ElementTree as etree\n",
        "from lxml import etree\n",
        "from xml.dom import minidom\n",
        "from nltk import TweetTokenizer\n",
        "from nltk.stem import SnowballStemmer \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erNyJe2YnP0K"
      },
      "source": [
        "basepath = 'aclImdb'\n",
        "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "target = 'aclImdb_v1.tar.gz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h044TxkewMJN"
      },
      "source": [
        "#Do not use.\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl-u2E7lPUpB"
      },
      "source": [
        "#Ready the twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG09jVr6EOSF"
      },
      "source": [
        "consumer_key = '7qEpJx4udZrB3cJFSmcoEuhZO'\n",
        "consumer_secret = 'zVh9ftwfXUJfY4RmB1cCykzxsrhbEYeVzfBUxvDedpXQ8liWGL'\n",
        "bearer = 'AAAAAAAAAAAAAAAAAAAAAEfrPAEAAAAAvwPVyFgNneWzv2BGQY0cKhAXVYw%3DNNcdt3sw6qZlVBXmur7qYMjU8M1ITCT6uGCdEqDzNtBpJUeJ9o'\n",
        "access_token = '1387915262939303937-3xhcmOirF3pZgD61PvHZcgO0ny7SvD'\n",
        "access_token_secret = 'nyssnnk4R03tKoLxHdMDW841o210G0hrRUTaH6UXlnHXQ'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmQ-_qh1sPpH"
      },
      "source": [
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqXe0gj6vHyg"
      },
      "source": [
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQW6tw0sPaAc"
      },
      "source": [
        "#Get data from tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xte05rXbItz4"
      },
      "source": [
        "search_words = \"#CatShark\"\n",
        "data_since = \"2021-05-05\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgvkKuyLGd_R"
      },
      "source": [
        "#tweets = tw.Cursor(api.search, q= search_words, lang = \"en\", since = data_since).items(4000)\n",
        "#tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KzyOkSnNjRI"
      },
      "source": [
        "#data_frame = [[tweet.user.screen_name, tweet.user.location, tweet.text] for tweet in tweets]\n",
        "#tw_df = pd.DataFrame(data = data_frame, columns= [\"User\", \"Location\", \"Text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8rlL5gnRoTr"
      },
      "source": [
        "#tw_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUgUpIMqPnKe"
      },
      "source": [
        "#Save the data in Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrkXBPw6SSQQ"
      },
      "source": [
        "#tw_df.to_csv(\"/content/drive/MyDrive/TweeterDataCSV/gura_unfiltered.csv\", index= False, encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyhRBK8sNAnf"
      },
      "source": [
        "vector= CountVectorizer(stop_words=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKasEQzjPsV5"
      },
      "source": [
        "#Normalize functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFtnwulAJ8C0"
      },
      "source": [
        "def normalize_laughs(message):\n",
        "  message = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[k])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[h])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(juas+|lol)\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0frJJwhLmoY"
      },
      "source": [
        "def process_twitter_features(message, twitter_features):\n",
        "\n",
        "  message = re.sub(r'[\\.\\,]http','. http', message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'[\\.\\,]#', '. #', message)\n",
        "  message = re.sub(r'[\\.\\,]@', '. @', message)\n",
        "\n",
        "  if twitter_features == REMOVE:\n",
        "    # eliminar menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))(@|#)\\S+', '', message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', '', message, flags=re.IGNORECASE)\n",
        "  elif twitter_features == NORMALIZE:\n",
        "    # cuando sea necesario se normalizaran las menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))@\\S+', MENTION, message)\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))#\\S+', HASHTAG, message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', URL, message, flags=re.IGNORECASE)\n",
        "\n",
        "  return message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu8KZo_OKfTQ"
      },
      "source": [
        "def preprocess(message):\n",
        "  message = message.lower()\n",
        "        \n",
        "  message = re.sub(r'(\\d+|\\n|\\brt\\b)', '', message)\n",
        "        \n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    message = re.sub(r'{0}'.format(s), t, message)\n",
        "        \n",
        "  message = re.sub(r'(.)\\1{2,}', r'\\1\\1', message)\n",
        "       \n",
        "  message = normalize_laughs(message)\n",
        "        \n",
        "  for s,t in SLANG:\n",
        "    message = re.sub(r'\\b{0}\\b'.format(s), t, message)\n",
        "\n",
        "  message = process_twitter_features(message, _twitter_features)\n",
        "\n",
        "  if _stemming:\n",
        "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
        "\n",
        "  return message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gslvx8mN8c3J"
      },
      "source": [
        "#Obtain corpus to train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "102it8TmO3k5"
      },
      "source": [
        "##List files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEOtZA5u8cLe"
      },
      "source": [
        "def files_of_path(path): \n",
        "    return [obj.name for obj in os.scandir(path) if obj.is_file()]\n",
        "    \n",
        "files= files_of_path(\"/content/drive/MyDrive/Tass_2017\")\n",
        "for file in files:\n",
        "    print(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zuuFRa0O5oo"
      },
      "source": [
        "##File to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g86PtL4hOB2Q"
      },
      "source": [
        "def list_to_csv(data, filename):\n",
        "  with open(filename, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',', lineterminator='\\n', quoting=csv.QUOTE_NONNUMERIC)\n",
        "    writer.writerows(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJqlZLiwPHWB"
      },
      "source": [
        "##CSV to list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oboAhZlOSCS"
      },
      "source": [
        "def csv_to_lists(filename):\n",
        "  messages = []\n",
        "  labels = []\n",
        "  with open(filename, 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "      messages.append(row[1])\n",
        "      labels.append(row[2])\n",
        "  return messages, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVwkdKlBP7la"
      },
      "source": [
        "#XML to df (General only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtCQE5BmOXz4"
      },
      "source": [
        "def general_tass_to_list(filename):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = tweet.find('sentiments/polarity/value').text\n",
        "    data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8PXuz1PQJND"
      },
      "source": [
        "def general_tass_2017_to_list(filename,qrel=None):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = qrel[tweetId]\n",
        "    data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VJ218wTSVEU"
      },
      "source": [
        "#Fuse general with it's sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtgjiBXxSMmt"
      },
      "source": [
        "def gold_standard_to_dict(filename):\n",
        "  with open(filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter='\\t')\n",
        "    data = {rows[0]: rows[1] for rows in reader}\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2KHBcKSSkDS"
      },
      "source": [
        "#Separate to train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uEZ0vLCSml3"
      },
      "source": [
        "def generate_train_test_subsets(data, size):\n",
        "  codes = [d[0] for d in data]\n",
        "  labels = [d[2] for d in data]\n",
        "  codes_train, codes_test, labels_train, labels_test = train_test_split(codes, labels, train_size=size)\n",
        "  train_data = [d for d in data if d[0] in codes_train]\n",
        "  test_data = [d for d in data if d[0] in codes_test]\n",
        "  return train_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4vOv0smS3px"
      },
      "source": [
        "#Execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TDdHJyITBom",
        "outputId": "75850625-6928-482c-9952-ddd6336af715"
      },
      "source": [
        "!git clone 'https://github.com/luisFernandoCastellanosG/Machine_learning'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Machine_learning' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWnqRdsuS6R_"
      },
      "source": [
        "data = []\n",
        "qrel = gold_standard_to_dict(\"/content/Machine_learning/Analisis_sentimientos_Twitter/espanish/datasets/Corpus/tass_2017/General Corpus of TASS/general-sentiment-3l.qrel\")\n",
        "data.extend(general_tass_2017_to_list(\"/content/Machine_learning/Analisis_sentimientos_Twitter/espanish/datasets/Corpus/tass_2017/General Corpus of TASS/general-tweets-test.xml\", qrel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9HwjUkYdjTf"
      },
      "source": [
        "#Define variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z29zCYymdTAR"
      },
      "source": [
        "NORMALIZE = 'normalize'\n",
        "REMOVE = 'remove'\n",
        "MENTION = 'twmention'\n",
        "HASHTAG = 'twhashtag'\n",
        "URL = 'twurl'\n",
        "LAUGH = 'twlaugh'\n",
        "_stemmer = SnowballStemmer('spanish')\n",
        "_tokenizer = TweetTokenizer().tokenize\n",
        "_twitter_features=\"normalize\"\n",
        "_stemming=False\n",
        "#Stuff to normalize\n",
        "DIACRITICAL_VOWELS = [('√°','a'), ('√©','e'), ('√≠','i'), ('√≥','o'), ('√∫','u'), ('√º','u')]\n",
        "SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
        "         ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'tambi√©n'),\n",
        "         ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GblWJdB0eIQH"
      },
      "source": [
        "#Functions to normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7xndIJfd2FP"
      },
      "source": [
        "def normalize_laughs(message):\n",
        "  message = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[k])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[h])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(juas+|lol)\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDlAOq1reVT-"
      },
      "source": [
        "def process_twitter_features(message, twitter_features):\n",
        "  message = re.sub(r'[\\.\\,]http','. http', message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'[\\.\\,]#', '. #', message)\n",
        "  message = re.sub(r'[\\.\\,]@', '. @', message)\n",
        "  if twitter_features == REMOVE:\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))(@|#)\\S+', '', message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', '', message, flags=re.IGNORECASE)\n",
        "  elif twitter_features == NORMALIZE:\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))@\\S+', MENTION, message)\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))#\\S+', HASHTAG, message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', URL, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs0udkU2es87"
      },
      "source": [
        "def preprocess(message):\n",
        "  message = message.lower()\n",
        "  message = re.sub(r'(\\d+|\\n|\\brt\\b)', '', message)\n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    message = re.sub(r'{0}'.format(s), t, message)\n",
        "  message = re.sub(r'(.)\\1{2,}', r'\\1\\1', message)\n",
        "  message = normalize_laughs(message)\n",
        "  for s,t in SLANG:\n",
        "    message = re.sub(r'\\b{0}\\b'.format(s), t, message)\n",
        "  message = process_twitter_features(message, _twitter_features)\n",
        "  if _stemming:\n",
        "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
        "\n",
        "  return message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLqFdbUBfd6q"
      },
      "source": [
        "#Download stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gw_M4jQfGbb",
        "outputId": "9219c409-68b8-4bc4-bbda-51fef59d0d43"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bkyaZUYyHvX"
      },
      "source": [
        "#Read data and refine it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee0paOCBxHei"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/luisFernandoCastellanosG/Machine_learning/master/Analisis_sentimientos_Twitter/espanish/datasets/Corpus/dataset_2017_full.csv', encoding='utf-8')\n",
        "#Index the data\n",
        "df.columns = ['Tweetid', 'Tweet','Sentiment']\n",
        "#Eliminate useless data\n",
        "df = df.drop(columns=\"Tweetid\")\n",
        "#Normalize the data\n",
        "df['Tweet'] = df['Tweet'].apply(preprocess)\n",
        "df.loc[df['Sentiment'] == 'NONE', 'Sentiment'] = '-1'\n",
        "df.loc[df['Sentiment'] == 'NEU', 'Sentiment'] = '0'\n",
        "df.loc[df['Sentiment'] == 'P', 'Sentiment'] = '1'\n",
        "df.loc[df['Sentiment'] == 'N', 'Sentiment'] = '2'\n",
        "df[\"Sentiment\"].unique()\n",
        "#Save to Drive\n",
        "df.to_csv('/content/drive/MyDrive/TweeterDataCSV/dataset_2017_refined.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGtuP59lykMg"
      },
      "source": [
        "#Tokenize/extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWRoWMjAyG5x"
      },
      "source": [
        "#Tokenize *and* clean\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "#Extract a document\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv)  # skip header\n",
        "        for line in csv:\n",
        "            text, label = line[:-3],  int(line[-2])\n",
        "            yield text, label\n",
        "#Return a number of documents\n",
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uehXWsmMzG0V"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-7xtPRVy_4H",
        "outputId": "cb67ffa2-7857-47d1-86fb-9242c66a96e3"
      },
      "source": [
        "path='/content/drive/MyDrive/TweeterDataCSV/dataset_2017_refined.csv'\n",
        "vect = HashingVectorizer(decode_error='ignore', \n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None, \n",
        "                         tokenizer=tokenizer)\n",
        "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
        "doc_stream = stream_docs(path)\n",
        "stop = stopwords.words('spanish')\n",
        "classes = np.array([-1,0, 1,2])\n",
        "for _ in range(50):\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
        "X_test = vect.transform(X_test)\n",
        "print('Precision: %.3f' % clf.score(X_test, y_test))\n",
        "clf = clf.partial_fit(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOIyXSio01vG"
      },
      "source": [
        "#Save to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on-SrwW6033f"
      },
      "source": [
        "dest = os.path.join('/content/drive/MyDrive/TweeterDataCSV/Classifier', 'pkl_objects')\n",
        "if not os.path.exists(dest):\n",
        "    os.makedirs(dest)\n",
        "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
        "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16CVjzNM1Q1_"
      },
      "source": [
        "Change the default file direction to Classifier (Why tho?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO_0AWYd1Z4G"
      },
      "source": [
        "#Why\n",
        "os.chdir('/content/drive/MyDrive/TweeterDataCSV/Classifier')\n",
        "#How\n",
        "#Much pain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5T4qe2W9nop"
      },
      "source": [
        "Deserialize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQCZSrxa9UTA"
      },
      "source": [
        "clf = pickle.load(open(os.path.join('/content/drive/MyDrive/TweeterDataCSV/Classifier/pkl_objects/', 'classifier.pkl'), 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJKH-qGXACE_"
      },
      "source": [
        "#Normalize/analyze the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "WFfYT05e9qIP",
        "outputId": "3cb2af5b-9cf8-4ce1-e245-b59add59c5c3"
      },
      "source": [
        "pbar = pyprind.ProgBar(50000)\n",
        "df = pd.read_csv('/content/drive/MyDrive/TweeterDataCSV/paro_nacional_raw_unfiltered.csv', encoding='utf-8')\n",
        "#creamos una columna llamada Sentimient donde guardaremos la predicci√≥n\n",
        "df['Sentiment'] =''\n",
        "#creamos una columna llamada Probability donde guardaremos la acertabilidad que dio el clasificador\n",
        "df['Probability']=0\n",
        "#conversi√≥n de sentimientos (numeros a palabras)= NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "label = {-1:'No feeling', 0:'Neutral', 1:'Positive',2: 'Negative'}\n",
        "for rowid in range(len(df.index)):\n",
        "  text=df['Text'][rowid]\n",
        "  textConvert = vect.transform([text]) \n",
        "  df['Sentiment'][rowid]=label[clf.predict(textConvert)[0]]\n",
        "  df['Probability'][rowid]=np.max(clf.predict_proba(textConvert))*100\n",
        "  pbar.update()\n",
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "0% [######                        ] 100% | ETA: 00:29:31"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Location</th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lecheconarro</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @tinixarianator: \"LE DOY 200MIL AL QUE ME A...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>_cringyj_</td>\n",
              "      <td>he/him</td>\n",
              "      <td>RT @Reytorre: Murillo llena por ambos carriles...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Burbuja86757842</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @AlirioUribeMuoz: Muy preocupante que la @P...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jasonripivi</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @angelamrobledo: Si est√°n probando censurar...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>YUDAIvaAdebutar</td>\n",
              "      <td>in the m√°gic shop</td>\n",
              "      <td>RT @nacisingracia_: Una de las mejores frases ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>moonchil_lu</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @ivntepop: üá®üá¥Aqu√≠ vamos, RESISTENCIA COLOMB...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DuxDumb</td>\n",
              "      <td>He!they‚ú™„ãõÔ∏é‚å´</td>\n",
              "      <td>RT @Lau96215173: @DalasReview @DalasReview esp...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>FrankAC9</td>\n",
              "      <td>Colombia</td>\n",
              "      <td>RT @LVMMOT7: Londres en este momento‚úäüèΩüá®üá¥ https...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gabrielapovedaa</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @Reytorre: Murillo llena por ambos carriles...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Yuriana09600948</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @Raspao__: SUCEDE EN BUGA, VALLE DEL CAUCA....</td>\n",
              "      <td>Positive</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>AndresEQuintero</td>\n",
              "      <td>Omicron Persei 8</td>\n",
              "      <td>RT @val_10o: ESTAN GASEANDO LOS BARRIOS!!! DON...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>luis14143</td>\n",
              "      <td>Frontino Ant</td>\n",
              "      <td>RT @jaiber5_3: #Anonymus ahora nos disparan de...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>CAR4JO</td>\n",
              "      <td>Buenos Aires, Argentina</td>\n",
              "      <td>RT @LVMMOT7: Londres en este momento‚úäüèΩüá®üá¥ https...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>FabinCeballos11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @Ulyssevengmail1: Colombianos en Londres sa...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Pirron_Esceptic</td>\n",
              "      <td>Colombia</td>\n",
              "      <td>RT @cutcolombia: #ParoNacional5M \\n\\nüìå En el m...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Kuromyee</td>\n",
              "      <td>Bogot√°</td>\n",
              "      <td>RT @cutcolombia: #ParoNacional5M \\n\\nüìå En el m...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>andermunoz_or</td>\n",
              "      <td>Isnos, Colombia</td>\n",
              "      <td>Mis fuerzas a todos lo que salen a marchar en ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Atziri48796384</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @LVMMOT7: Londres en este momento‚úäüèΩüá®üá¥ https...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>JuanEstebanTro4</td>\n",
              "      <td>Popay√°n, Colombia</td>\n",
              "      <td>RT @cerosetenta: As√≠ se ve la manifestaci√≥n de...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Mariam00995621</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @FisicoImpuro: URGENTE: Esto esta pasando e...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               User                 Location  ... Sentiment Probability\n",
              "0      lecheconarro                      NaN  ...  Positive          76\n",
              "1         _cringyj_                   he/him  ...  Positive          68\n",
              "2   Burbuja86757842                      NaN  ...  Negative          49\n",
              "3       Jasonripivi                      NaN  ...  Negative          53\n",
              "4   YUDAIvaAdebutar        in the m√°gic shop  ...  Positive          71\n",
              "5       moonchil_lu                      NaN  ...  Positive          71\n",
              "6           DuxDumb              He!they‚ú™„ãõÔ∏é‚å´  ...  Positive          63\n",
              "7          FrankAC9                 Colombia  ...  Positive          68\n",
              "8   gabrielapovedaa                      NaN  ...  Positive          68\n",
              "9   Yuriana09600948                      NaN  ...  Positive          65\n",
              "10  AndresEQuintero         Omicron Persei 8  ...  Negative          48\n",
              "11        luis14143             Frontino Ant  ...  Positive          65\n",
              "12           CAR4JO  Buenos Aires, Argentina  ...  Positive          68\n",
              "13  FabinCeballos11                      NaN  ...  Negative          54\n",
              "14  Pirron_Esceptic                 Colombia  ...  Positive          60\n",
              "15         Kuromyee                   Bogot√°  ...  Positive          60\n",
              "16    andermunoz_or          Isnos, Colombia  ...  Positive          61\n",
              "17   Atziri48796384                      NaN  ...  Positive          68\n",
              "18  JuanEstebanTro4        Popay√°n, Colombia  ...  Positive          58\n",
              "19   Mariam00995621                      NaN  ...  Positive          74\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FltS0TQhAPLS"
      },
      "source": [
        "Save to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0aCRWqp__zM"
      },
      "source": [
        "df.to_csv('/content/drive/MyDrive/TweeterDataCSV/paro_nacional_treated_filtered.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGgu7S9eDn6R"
      },
      "source": [
        "This also analyzes the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tObCcOrqA1ya"
      },
      "source": [
        "def f_prediction(row):\n",
        "  text=row['Text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return label[clf.predict(textConvert)[0]]\n",
        "\n",
        "def f_probability(row):\n",
        "  text=row['Text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return np.max(clf.predict_proba(textConvert))*100\n",
        "\n",
        "df[\"Sentiment\"] = df.apply(f_prediction, axis=1)\n",
        "df[\"Probability\"] = df.apply(f_probability, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "vY5gcwGPDnAl",
        "outputId": "b3469a4e-31ae-423b-fd76-bb8ac99fd60e"
      },
      "source": [
        "df.groupby('Sentiment')['Location'].nunique().plot(kind='bar')\n",
        "print(df.groupby(['Sentiment']).size())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment\n",
            "Negative    1584\n",
            "Positive    8416\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}